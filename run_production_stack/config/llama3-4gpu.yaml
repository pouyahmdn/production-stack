servingEngineSpec:
  runtimeClassName: ""
  modelSpec:
  - name: "llama3"
    repository: "vllm/vllm-openai"
    tag: "latest"
    modelURL: "meta-llama/Meta-Llama-3-8B-Instruct"
    replicaCount: 4

    requestCPU: 10
    requestMemory: "23000Mi"
    requestGPU: 1

    pvcStorage: "50Gi"
    pvcAccessMode:
      - ReadWriteOnce

    vllmConfig:
      enableChunkedPrefill: false
      enablePrefixCaching: false
      maxModelLen: 8192
      dtype: "bfloat16"
      extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.97", "--no-enable-prefix-caching"]

    hf_token: "UNSET"
routerSpec:
  repository: "pouyah/vllm_router_custom"
  tag: "latest"
  routingLogic: "custom"
  imagePullPolicy: "Never"